{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'weather.csv' does not exist: b'weather.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1d6b87ecd325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weather.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'weather.csv' does not exist: b'weather.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('weather.csv')\n",
    "data['date']=pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('weather.csv')\n",
    "data = data.loc[:,['station_nbr','date','codesum']]\n",
    "data['date']=pd.to_datetime(data['date'])\n",
    "\n",
    "# Add variables\n",
    "data['is_weekend'] = data['date'].dt.dayofweek.apply(lambda x: int(x in [5,6]))\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Add is_holiday\n",
    "from datetime import datetime\n",
    "date_format = \"%Y-%m-%d\"\n",
    "\n",
    "holiday_list = [\"2012-01-01\", \"2012-01-16\", \"2012-02-14\", \"2012-02-20\", \"2012-04-08\",\n",
    "                \"2012-05-13\", \"2012-05-28\", \"2012-06-17\", \"2012-07-04\", \"2012-09-03\",\n",
    "                \"2012-10-08\", \"2012-10-31\", \"2012-11-11\", \"2012-11-20\", \"2012-11-21\",\n",
    "                \"2012-11-22\", \"2012-11-23\", \"2012-11-24\", \"2012-11-25\", \"2012-11-26\",\n",
    "                \"2012-12-24\", \"2012-12-25\", \"2012-12-31\", \"2013-01-01\", \"2013-01-21\",\n",
    "                \"2013-02-14\", \"2013-02-18\", \"2013-05-31\", \"2013-05-12\", \"2013-05-27\",\n",
    "                \"2013-06-16\", \"2013-07-04\", \"2013-09-02\", \"2013-10-14\", \"2013-10-31\",\n",
    "                \"2013-11-11\", \"2013-11-26\", \"2013-11-27\", \"2013-11-28\", \"2013-11-29\",\n",
    "                \"2013-11-30\", \"2013-12-01\", \"2013-12-02\", \"2013-12-24\", \"2013-12-25\",\n",
    "                \"2013-12-31\", \"2014-01-01\", \"2014-01-20\", \"2014-02-14\", \"2014-02-17\",\n",
    "                \"2014-04-20\", \"2014-05-11\", \"2014-05-26\", \"2014-06-15\", \"2014-07-04\",\n",
    "                \"2014-09-01\", \"2014-10-13\", \"2014-10-31\"]\n",
    "\n",
    "holidays = [datetime.strptime(holiday, date_format) for holiday in holiday_list]\n",
    "\n",
    "data['is_holiday'] = 0\n",
    "\n",
    "for i in range(0, data.shape[0]):\n",
    "    if data[\"date\"][i] in holidays:\n",
    "        data.is_holiday.iloc[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(txt):\n",
    "    \n",
    "    #remove punctuation\n",
    "    sent = re.sub('[^a-zA-Z]', ' ', txt)\n",
    "    sent = re.sub(r\"\\s+\", \"\", sent)\n",
    "    words = []\n",
    "    for i in range(0, len(sent), 2):\n",
    "        \n",
    "        words.append(sent[i:i+2]) if sent[i:i+2] not in words else None\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer = text_process, max_features=33)\n",
    "codesum = cv.fit_transform(data.codesum)\n",
    "condesum_df = pd.DataFrame(codesum.toarray(),index=data.index,columns=sorted(cv.vocabulary_.keys()))\n",
    "data = data.drop('codesum', axis = 1)\n",
    "data = pd.concat([condesum_df, data], axis = 1)\n",
    "del codesum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.date = pd.to_datetime(train.date)\n",
    "key = pd.read_csv('key.csv')\n",
    "data = pd.merge(data, key, how = 'left', on = 'station_nbr')\n",
    "training = pd.merge(data,train,how = 'right', on = ['date', 'store_nbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.date = pd.to_datetime(test.date)\n",
    "testing = pd.merge(data,test,how = 'right', on = ['date', 'store_nbr'])\n",
    "ytrain = training['units']\n",
    "xtrain = training.drop(['station_nbr','date', 'units'], axis = 1)\n",
    "xtest = testing.drop(['station_nbr', 'date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 45\n",
      "2 / 45\n",
      "3 / 45\n",
      "4 / 45\n",
      "5 / 45\n",
      "6 / 45\n",
      "7 / 45\n",
      "8 / 45\n",
      "9 / 45\n",
      "10 / 45\n",
      "11 / 45\n",
      "12 / 45\n",
      "13 / 45\n",
      "14 / 45\n",
      "15 / 45\n",
      "16 / 45\n",
      "17 / 45\n",
      "18 / 45\n",
      "19 / 45\n",
      "20 / 45\n",
      "21 / 45\n",
      "22 / 45\n",
      "23 / 45\n",
      "24 / 45\n",
      "25 / 45\n",
      "26 / 45\n",
      "27 / 45\n",
      "28 / 45\n",
      "29 / 45\n",
      "30 / 45\n",
      "31 / 45\n",
      "32 / 45\n",
      "33 / 45\n",
      "34 / 45\n",
      "35 / 45\n",
      "36 / 45\n",
      "37 / 45\n",
      "38 / 45\n",
      "39 / 45\n",
      "40 / 45\n",
      "41 / 45\n",
      "42 / 45\n",
      "43 / 45\n",
      "44 / 45\n",
      "45 / 45\n"
     ]
    }
   ],
   "source": [
    "for store_nbr in range(1, 46):\n",
    "    print(store_nbr, \"/ 45\")\n",
    "    for item_nbr in range(1, 112):\n",
    "        # extract one specific comb of store nbr and item nbr\n",
    "        temp_data = training[(training[\"store_nbr\"] == store_nbr) & (training[\"item_nbr\"] == item_nbr)]\n",
    "        \n",
    "        # check if all units in the temp data are zeros\n",
    "        if np.mean(temp_data[\"units\"] == 0) == 1.0:\n",
    "            \n",
    "            # delete all indices in the temp data from the whole data\n",
    "            training = training.drop(axis = 0, index = temp_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.to_csv(\"training_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_list = []\n",
    "saved_list = []\n",
    "for store_nbr in range(1, 46):\n",
    "    for item_nbr in range(1, 112):\n",
    "        # extract one specific comb of store nbr and item nbr\n",
    "        temp_data = training[(training[\"store_nbr\"] == store_nbr) & (training[\"item_nbr\"] == item_nbr)]\n",
    "        \n",
    "        if temp_data.shape[0] == 0:\n",
    "            deleted_list.append((store_nbr, item_nbr))\n",
    "        else:\n",
    "            saved_list.append((store_nbr, item_nbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save deleted_list as a file\n",
    "with open(\"deleted_list.txt\", \"wb\") as dl:\n",
    "    pickle.dump(deleted_list, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save saved_list as a file\n",
    "with open(\"saved_list.txt\", \"wb\") as sl:\n",
    "    pickle.dump(saved_list, sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to load lists \n",
    "with open(\"deleted_list.txt\", \"rb\") as dl:\n",
    "    deleted_list = pickle.load(dl)\n",
    "\n",
    "with open(\"saved_list.txt\", \"rb\") as sl:\n",
    "    saved_list = pickle.load(sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "xtrain1 = csr_matrix(xtrain.values,shape = xtrain.shape, dtype=float)\n",
    "xtest1 = csr_matrix(xtest.values,shape = xtest.shape,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor().fit(xtrain, ytrain)\n",
    "ypred = gb.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(\"output.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
